{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d31c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlos/miniconda3/envs/catRegression/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-16 14:44:57.189232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, OneHotEncoder, MultiLabelBinarizer\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout,\\\n",
    "        Embedding, Concatenate, LayerNormalization, MultiHeadAttention, \\\n",
    "        GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf88aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "def oversample_high_values(X, y, threshold, factor=3):\n",
    "    \"\"\"Triplicate samples where target y is above threshold\"\"\"\n",
    "    log_threshold = np.log(threshold + 1)\n",
    "    log_upper_bound = np.log(MAX_SOLD_PRICE * 1.3 + 1)\n",
    "\n",
    "    mask_high = (y > log_threshold)\n",
    "    mask_high_n_low = (y > log_threshold) & (y < log_upper_bound)\n",
    "    y_high = y[mask_high]\n",
    "    y_high_n_low = y[mask_high_n_low]\n",
    "    X_high = [x[mask_high] for x in X]\n",
    "    X_high_n_low = [x[mask_high_n_low] for x in X]\n",
    "\n",
    "    int_part = int(factor)\n",
    "    frac_part = factor - int_part\n",
    "\n",
    "    X_oversampled = []\n",
    "    for i in range(len(X)):\n",
    "        x_orig = X[i]\n",
    "        x_hi = X_high[i]\n",
    "        x_hi_n_lo = X_high_n_low[i]\n",
    "\n",
    "        if frac_part > 0 and len(x_hi_n_lo) > 0:\n",
    "            n_extra = int(len(x_hi_n_lo) * frac_part)\n",
    "            sampled_indices = np.random.choice(len(x_hi_n_lo), n_extra, replace=False)\n",
    "            x_extra = x_hi_n_lo[sampled_indices]\n",
    "            x_aug = np.concatenate([x_orig] + [x_hi] * int_part + [x_extra], axis=0)\n",
    "        else:\n",
    "            x_aug = np.concatenate([x_orig] + [x_hi] * int_part, axis=0)\n",
    "\n",
    "        X_oversampled.append(x_aug)\n",
    "\n",
    "    if frac_part > 0 and len(y_high_n_low) > 0:\n",
    "        n_extra = int(len(y_high_n_low) * frac_part)\n",
    "        sampled_indices = np.random.choice(len(y_high_n_low), n_extra, replace=False)\n",
    "        y_extra = y_high_n_low[sampled_indices]\n",
    "        y_oversampled = np.concatenate([y] + [y_high] * int_part + [y_extra])\n",
    "    else:\n",
    "        y_oversampled = np.concatenate([y] + [y_high] * int_part)\n",
    "\n",
    "    return X_oversampled, y_oversampled\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.3):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"weighted_mse_original_space\")\n",
    "def weighted_mse_original_space(y_true, y_pred):\n",
    "    cutoff = tf.constant(10, dtype=tf.float32)  \n",
    "    sharpness = tf.constant(1, dtype=tf.float32)\n",
    "    y_true_orig = tf.math.expm1(y_true)\n",
    "    y_pred_orig = tf.math.expm1(y_pred)\n",
    "\n",
    "    weights = 1.0 / (1.0 + tf.exp(sharpness * (y_true_orig - cutoff)))\n",
    "\n",
    "    weighted_error = weights * tf.square(y_true_orig - y_pred_orig)\n",
    "    return weighted_error\n",
    "        \n",
    "\n",
    "def build_mixed_transformer_model(cat_cardinalities, num_numerical_features, item_embed_dim,\n",
    "                                  embed_dim=16, num_heads=2, ff_dim=64, num_layers=2, dropout_rate=0.3):\n",
    "    inputs = []\n",
    "    cat_embeds = []\n",
    "\n",
    "    # Categorical inputs\n",
    "    for cardinality in cat_cardinalities:\n",
    "        inp = Input(shape=(1,), dtype=\"int32\")\n",
    "        emb = Embedding(input_dim=cardinality + 1, output_dim=embed_dim)(inp)\n",
    "        emb = Flatten()(emb)\n",
    "        inputs.append(inp)\n",
    "        cat_embeds.append(emb)\n",
    "\n",
    "    # Items input (sequence)\n",
    "    item_input = Input(shape=(None,), dtype=\"int32\", name=\"items_input\")\n",
    "    item_emb = Embedding(input_dim=item_embed_dim, output_dim=embed_dim)(item_input)  # Adjust vocab size\n",
    "    x_item = item_emb\n",
    "    for _ in range(num_layers):\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x_item, x_item)\n",
    "        x_item = LayerNormalization(epsilon=1e-6)(x_item + attn_output)\n",
    "        ffn_output = Dense(ff_dim, activation='relu')(x_item)\n",
    "        ffn_output = Dense(embed_dim)(ffn_output)\n",
    "        x_item = LayerNormalization(epsilon=1e-6)(x_item + ffn_output)\n",
    "    item_pooled = GlobalAveragePooling1D()(x_item)\n",
    "    inputs.append(item_input)\n",
    "\n",
    "    # Numerical input\n",
    "    num_input = Input(shape=(num_numerical_features,), dtype=\"float32\")\n",
    "    inputs.append(num_input)\n",
    "\n",
    "    # Concatenate all\n",
    "    x = Concatenate()(cat_embeds + [item_pooled, num_input])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output = Dense(1)(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "exp_clip = lambda np_array: np.clip(np.exp(np_array) - 1, None, MAX_SOLD_PRICE)\n",
    "def show_model_performance( model, X_test, y_test):\n",
    "    \"\"\" Shows model's performance metrics, visualization of predicted against actual results\n",
    "    and residuals against predicted price.\n",
    "    Input: model, testing data\n",
    "    Output: predictions on the testing data for further inspection.\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    predictions_original = exp_clip(predictions) \n",
    "    y_test_original = exp_clip(y_test) \n",
    "    #compute residuals\n",
    "    residuals = y_test_original - predictions_original\n",
    "    \n",
    "    mean_res = np.sum( residuals)/len(residuals)\n",
    "    std_res = np.sqrt(np.sum((residuals - mean_res) ** 2)/len(residuals))\n",
    "    skew_res = skew(residuals, bias=False)\n",
    "    kurt_res = kurtosis(residuals, bias=False)\n",
    "    \n",
    "    # Weighted metrics\n",
    "    mae = np.mean(np.sum(np.abs(residuals)))/len(y_test)\n",
    "    mse = np.sum(residuals**2)/len(y_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(\"\\nResiduals Statistics:\")\n",
    "    print(f\"Mean: {mean_res:.4f}\")\n",
    "    print(f\"Std Dev: {std_res:.4f}\")\n",
    "    print(f\"Skewness: {skew_res:.4f}\")\n",
    "    print(f\"Kurtosis: {kurt_res:.4f}\")\n",
    "    \n",
    "    print(\"\\nEvaluation Metrics (on original sold_price scale):\")\n",
    "    print(f\"MAE: ${mae:.4f}\")\n",
    "    print(f\"MSE: ${mse:.4f}\")\n",
    "    print(f\"RMSE: ${rmse:.4f}\")\n",
    "    \n",
    "    plt.scatter(predictions_original, residuals, alpha=0.2)\n",
    "    plt.axhline(0, color=\"red\", linestyle=\"--\", lw=2)\n",
    "    plt.xlabel(\"Predicted Sold Price\")\n",
    "    plt.ylabel(\"Residual  (Actual âˆ’ Predicted)\")\n",
    "    plt.title(\"Residuals vs. Predicted Sold Price\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_test_original,predictions_original,alpha=0.4,c='blue',label=\"Predictions\")\n",
    "    \n",
    "    # Ideal fit line\n",
    "    min_val = min(y_test_original.min(), predictions_original.min())\n",
    "    max_val = max(y_test_original.max(), predictions_original.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label=\"Ideal Fit\")\n",
    "    \n",
    "    plt.xlabel(\"Actual Sold Price\")\n",
    "    plt.ylabel(\"Predicted Sold Price\")\n",
    "    plt.title(\"Actual vs Predicted Sold Price\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.hist(residuals, bins=80, color='gray')\n",
    "    plt.title(\"Distribution of Residuals\")\n",
    "    plt.xlabel(\"Residual\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "MAX_SOLD_PRICE = 8.15\n",
    "df = pd.read_csv(\"./training_data.csv\")\n",
    "df=df[df[\"sold_price\"] < MAX_SOLD_PRICE*1.2 ]\n",
    "#For the read data perform the relevant transformations on the numerical variables,\n",
    "#transform the categorical variables using\n",
    "categoric_features = [\"pay_frequency\",\"income_type\", \"source_subid\", \"state\",\"day_hour_cross\"]#, \"prefiltered_orders\"]\n",
    "numeric_features = [\"annual_income\", \"requested_amount\", \"sold_price\"]\n",
    "\n",
    "#transformation to some numerical columns\n",
    "\n",
    "for col in numeric_features:\n",
    "    if col != \"requested_amount\":\n",
    "        df[col+'_log']=np.log(df[col]+1)\n",
    "\n",
    "\n",
    "#fill nas in cat features\n",
    "for cat in categoric_features:\n",
    "    df[cat] = df[cat].fillna('missing')\n",
    "\n",
    "numeric_features = [ c for c in numeric_features if c!='sold_price']\n",
    "numeric_features.append('annual_income_log')\n",
    "\n",
    "print(f\"Updated numeric_features: {numeric_features}\")\n",
    "\n",
    "df[\"items\"]=df[\"prefiltered_orders\"].apply(lambda x: [int(y) for y in x.split(\";\")])\n",
    "#Flatten all item lists\n",
    "\n",
    "all_items = [item for sublist in df['items'] for item in sublist]\n",
    "le = LabelEncoder()\n",
    "le.fit(all_items)\n",
    "\n",
    "# Map each list to a list of integers\n",
    "df['items_encoded'] = df['items'].apply(lambda lst: le.transform(lst))\n",
    "#padding the sequences\n",
    "max_len = df[\"items_encoded\"].apply(len).max()\n",
    "X_items = pad_sequences(df['items_encoded'], maxlen=max_len, padding='post')\n",
    "X_num = df[numeric_features].values\n",
    "\n",
    "\n",
    "col_medians = np.nanmedian(X_num, axis=0)\n",
    "#localize nas \n",
    "inds = np.where(np.isnan(X_num))\n",
    "#replace\n",
    "X_num[inds] = np.take(col_medians, inds[1])\n",
    "cat_encoders = {}\n",
    "X_cat = []\n",
    "\n",
    "for col in categoric_features:\n",
    "    le = LabelEncoder()\n",
    "    df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "    cat_encoders[col] = le\n",
    "    X_cat.append(df[col + '_encoded'].values.reshape(-1, 1)) \n",
    "\n",
    "cat_cardinalities = [df[col].nunique() for col in categoric_features]\n",
    "y_target = df[\"sold_price_log\"].values\n",
    "\n",
    "\n",
    "#train-test-val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_samples = X_cat[0].shape[0]\n",
    "indices = np.arange(n_samples)\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(val_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "X_cat_tr = [x[train_idx] for x in X_cat]\n",
    "X_cat_v = [x[val_idx] for x in X_cat]\n",
    "X_cat_tst =  [x[test_idx] for x in X_cat]\n",
    "\n",
    "X_items_tr = X_items[train_idx]\n",
    "X_items_v = X_items[val_idx]\n",
    "X_items_tst = X_items[test_idx]\n",
    "\n",
    "X_num_tr = X_num[train_idx]\n",
    "X_num_v = X_num[val_idx]\n",
    "X_num_tst = X_num[test_idx]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_num_tr)\n",
    "\n",
    "X_num_tr = scaler.transform(X_num_tr)\n",
    "X_num_v = scaler.transform(X_num_v)\n",
    "X_num_tst = scaler.transform(X_num_tst)\n",
    "\n",
    "# Create model input lists\n",
    "X_tr = X_cat_tr + [X_items_tr, X_num_tr]\n",
    "X_v = X_cat_v + [X_items_v, X_num_v]\n",
    "X_tst = X_cat_tst + [X_items_tst, X_num_tst]\n",
    "\n",
    "y_tr = y_target[train_idx]\n",
    "y_v = y_target[val_idx]\n",
    "y_tst = y_target[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diff_items = len(set(all_items))\n",
    "\n",
    "def objective(trial):\n",
    "    global X_tr, y_tr, X_v, y_v\n",
    "    # Architecture hyperparameters\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", 48, 120)\n",
    "    num_heads =  trial.suggest_categorical(\"num_heads\", [1, 2, 4, 8]) \n",
    "    ff_dim = trial.suggest_int(\"ff_dim\", 128, 192)\n",
    "    item_embed_dim = trial.suggest_int(\"item_embed_dim\", n_diff_items +1, 5000)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.4)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n",
    "\n",
    "    # Training callback hyperparameters\n",
    "    early_stopping_patience = trial.suggest_int(\"early_stopping_patience\", 3, 10)\n",
    "    reduce_lr_patience = trial.suggest_int(\"reduce_lr_patience\", 3, 10)\n",
    "\n",
    "    # Data processing constants\n",
    "    #proportion_cut = trial.suggest_float(\"proportion_cut\", 1.0, 3.0)\n",
    "    factor_augmentation = trial.suggest_float(\"factor_augmentation\", 0.0, 3.0)\n",
    "    low_sold_price = trial.suggest_float(\"low_sold_price\", 5.5, 7.0)\n",
    "\n",
    "\n",
    "    X_tr, y_tr = oversample_high_values(X_tr, y_tr, low_sold_price, factor_augmentation)\n",
    "\n",
    "    # ----- Model construction -----\n",
    "    model = build_mixed_transformer_model(\n",
    "        cat_cardinalities=cat_cardinalities,\n",
    "        num_numerical_features=X_num.shape[1],\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=num_heads,\n",
    "        item_embed_dim = item_embed_dim,\n",
    "        ff_dim=ff_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=weighted_mse_original_space\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_v, y_v),\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=reduce_lr_patience, verbose=1),\n",
    "            tf.keras.callbacks.EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    predictions = model.predict(X_tst).flatten()\n",
    "    predictions_original = exp_clip(predictions)\n",
    "    y_test_original = exp_clip(y_tst)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_original, predictions_original))\n",
    "\n",
    "    max_pred = np.max(predictions_original)\n",
    "    penalty = max(0, 8.15 - max_pred)**2\n",
    "\n",
    "    return rmse + penalty\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"optuna_class_embedings\",  # Custom name for this study\n",
    "    storage=\"sqlite:///optuna_study.db\",  # File-based SQLite DB\n",
    "    load_if_exists=True  # Will not overwrite if it exists\n",
    ")\n",
    "\n",
    "# Run the optimization and store everything\n",
    "study.optimize(objective, n_trials=20, n_jobs=1, catch=(MemoryError,))\n",
    "\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "catRegression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
